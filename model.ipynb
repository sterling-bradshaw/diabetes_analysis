{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset \n",
    "cdc_diabetes_health_indicators = fetch_ucirepo(id=891)\n",
    "\n",
    "# data (as pandas dataframes) \n",
    "df = cdc_diabetes_health_indicators.data.features\n",
    "df[\"Diabetes\"] = cdc_diabetes_health_indicators.data.targets\n",
    "X = df.drop(columns='Diabetes')\n",
    "y = cdc_diabetes_health_indicators.data.targets\n",
    "\n",
    "# Create df with features and target for graphing purposes later\n",
    "\n",
    "\n",
    "# Jitter y values for graphing purposes later\n",
    "y_jitter = []\n",
    "for i in range(len(y)):\n",
    "    y_jitter.append(y.loc[i][0] + np.random.uniform(-0.2,0.2))\n",
    "\n",
    "# feature engineering\n",
    "genhlth_dict = {1: 5, 2: 4, 3: 3, 4: 2, 5: 1}\n",
    "X['GenHlth'] = X['GenHlth'].map(genhlth_dict)\n",
    "age_dict = {1: 21, 2: 27, 3: 32, 4: 37, 5: 42, 6: 47, 7: 52, 8: 57, 9: 62, 10: 67, 11: 72, 12: 77, 13: 80}\n",
    "X['Age'] = X['Age'].map(age_dict)\n",
    "income_dict = {1: 10, 2: 12.5, 3: 17.5, 4: 22.5, 5: 30, 6: 42.5, 7: 62.5, 8: 75}\n",
    "X['Income'] = X['Income'].map(income_dict)\n",
    "X['MentHlth'] = 30 - X['MentHlth']\n",
    "X['PhysHlth'] = 30 - X['PhysHlth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NA\n",
    "print(X.isna().sum())\n",
    "print(y.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Five Number Summaries of numeric variables\n",
    "print(\"BMI Summary: \", np.percentile(X[\"BMI\"], [0, 25, 50, 75, 100]))\n",
    "print(\"MentHlth Summary: \", np.percentile(X[\"MentHlth\"], [0, 25, 50, 75, 100]))\n",
    "print(\"PhysHlth Summary: \", np.percentile(X[\"PhysHlth\"], [0, 25, 50, 75, 100]))\n",
    "print(\"Age Summary: \", np.percentile(X[\"Age\"], [0, 25, 50, 75, 100]))\n",
    "print(\"Education Summary: \", np.percentile(X[\"Education\"], [0, 25, 50, 75, 100]))\n",
    "print(\"Income Summary: \", np.percentile(X[\"Income\"], [0, 25, 50, 75, 100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bmi = []\n",
    "for i in range(len(X[\"BMI\"])):\n",
    "    bmi.append(X[\"BMI\"][i] + np.random.uniform(-0.2,0.2))\n",
    "# Diabetes vs. BMI\n",
    "plt.figure(figsize=(11,6))\n",
    "plt.scatter(bmi, y_jitter, s=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ment = []\n",
    "for i in range(len(X[\"MentHlth\"])):\n",
    "    ment.append(X[\"MentHlth\"][i] + np.random.uniform(-0.2,0.2))\n",
    "plt.figure(figsize=(11,6))\n",
    "plt.scatter(ment, y_jitter, s = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phys = []\n",
    "for i in range(len(X[\"PhysHlth\"])):\n",
    "    phys.append(X[\"PhysHlth\"][i] + np.random.uniform(-0.2,0.2))\n",
    "plt.figure(figsize=(11,6))    \n",
    "plt.scatter(phys, y_jitter, s = 0.1)\n",
    "plt.xlabel(\"Number of Good Physical Health Days (Last 30)\")\n",
    "plt.ylabel(\"Diabetes\")\n",
    "plt.title(\"Physical Health vs. Diabetes Diagnosis\")\n",
    "plt.savefig(\"physhlth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Age\", hue=\"Diabetes\", data=df)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Diabetes by Age', fontsize = 14)\n",
    "#plt.savefig('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Education\", hue=\"Diabetes\", data=df)\n",
    "plt.xlabel('Education Level')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Diabetes by Education Level', fontsize = 14)\n",
    "#plt.savefig(\"edu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Income\", hue=\"Diabetes\", data=df)\n",
    "plt.xlabel('Income (thousands of $)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Diabetes by Income', fontsize = 14)\n",
    "plt.savefig('income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train/validation/test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.2)\n",
    "\n",
    "# Ravel y's to avoid DataConversionWarning\n",
    "y_train = np.array(y_train)\n",
    "y_train = y_train.ravel()\n",
    "y_test = np.array(y_test)\n",
    "y_test = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Logistic Regression Model\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_train, y_train)\n",
    "train_preds = log.predict(X_train)\n",
    "test_preds = log.predict(X_test)\n",
    "y_pred_proba = log.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# View metrics\n",
    "print(\"AUC: \", auc)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_preds))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_preds))\n",
    "print(\"F1 Score: \", f1_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Random Forest Model\n",
    "rf = RandomForestClassifier(min_samples_split=30)\n",
    "rf.fit(X_train, y_train)\n",
    "test_preds = rf.predict(X_test)\n",
    "train_preds = rf.predict(X_train)\n",
    "y_pred_proba = rf.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_preds))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_preds))\n",
    "print(\"F1 Score: \", f1_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Gradient Boosting Model\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, y_train)\n",
    "test_preds = gb.predict(X_test)\n",
    "train_preds = gb.predict(X_train)\n",
    "y_pred_proba = gb.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_preds))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_preds))\n",
    "print(\"F1 Score: \", f1_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Ada Boost Model\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n",
    "test_preds = ada.predict(X_test)\n",
    "train_preds = ada.predict(X_train)\n",
    "y_pred_proba = ada.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_preds))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_preds))\n",
    "print(\"F1 Score: \", f1_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Extra Trees Model\n",
    "extra = ExtraTreesClassifier(min_samples_split=20)\n",
    "extra.fit(X_train, y_train)\n",
    "test_preds = extra.predict(X_test)\n",
    "train_preds = extra.predict(X_train)\n",
    "y_pred_proba = extra.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_preds))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_preds))\n",
    "print(\"F1 Score: \", f1_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit KNN Classifier Model\n",
    "# NOTE: This model ran for hours and never finished for me\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "test_preds = knn.predict(X_test)\n",
    "train_preds = knn.predict(X_train)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_preds))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_preds))\n",
    "print(\"F1 Score: \", f1_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Support Vector Classifier Model\n",
    "# NOTE: This model ran for hours and never finished for me\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "test_preds = svc.predict(X_test)\n",
    "train_preds = svc.predict(X_train)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_preds))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_preds))\n",
    "print(\"F1 Score: \", f1_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Gauusian Naive Bayes Model\n",
    "gaus = GaussianNB()\n",
    "gaus.fit(X_train, y_train)\n",
    "test_preds = gaus.predict(X_test)\n",
    "train_preds = gaus.predict(X_train)\n",
    "y_pred_proba = gaus.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"AUC: \", auc)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_preds))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_preds))\n",
    "print(\"F1 Score: \", f1_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pipe and parameter grid\n",
    "pipe = pipe = Pipeline([\n",
    "    ('grad', GradientBoostingClassifier())\n",
    "])\n",
    "params = {\n",
    "  'grad__loss':('log_loss','exponential'), \n",
    "  'grad__learning_rate':(0.05, 0.1, 0.2),\n",
    "  'grad__n_estimators':(50, 100, 150, 200),\n",
    "  'grad__criterion':('friedman_mse','squared_error')\n",
    "}\n",
    "\n",
    "# Run the grid search and output the best hyperparameters\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring='accuracy', cv=10)\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit new model with tuned hyperparameters\n",
    "gb = GradientBoostingClassifier(criterion=\"friedman_mse\",\n",
    "                                learning_rate=0.2,\n",
    "                                loss='exponential',\n",
    "                                n_estimators=200)\n",
    "gb.fit(X_train, y_train)\n",
    "preds = gb.predict(X_test)\n",
    "y_pred_proba = gb.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print(\"AUC: \", auc)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=32,\n",
    "                                        max_value=512,\n",
    "                                        step=32),\n",
    "                           activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "                    hp.Choice('learning_rate',\n",
    "                              values=[1e-2, 1e-3, 1e-4])),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning the hyperparameters via a random search\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    directory=\"ANN_hyperparameter_tuning\")\n",
    "\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs = 5,\n",
    "             validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outputs the best hyperparameters\n",
    "print(\"Best # of neurons in dense layer: \", tuner.get_best_hyperparameters(num_trials=1)[0].get('units'))\n",
    "print(\"Best Learning Rate: \", tuner.get_best_hyperparameters(num_trials=1)[0].get('learning_rate'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit logistic regression model\n",
    "log = LogisticRegression(max_iter=1000)\n",
    "log.fit(X_train, y_train)\n",
    "train_preds = log.predict(X_train)\n",
    "test_preds = log.predict(X_test)\n",
    "y_pred_proba = log.predict_proba(X_test)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "# Get all metrics from logistic regression model\n",
    "print(\"AUC: \", auc)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, train_preds))\n",
    "print(\"Test Accuracy: \", accuracy_score(y_test, test_preds))\n",
    "print(\"F1 Score: \", f1_score(y_test, test_preds))\n",
    "print(recall_score(y_test, test_preds))\n",
    "print(precision_score(y_test, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and plot SHAP values for each factor\n",
    "explainer = shap.TreeExplainer(gb)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "exp = shap.Explanation(\n",
    "    values=shap_values,\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_train,\n",
    "    feature_names=X.columns\n",
    ")\n",
    "shap.plots.beeswarm(exp, show=False, color_bar=False)\n",
    "plt.colorbar()\n",
    "plt.title(\"SHAP Values from GradientBoosing Model\")\n",
    "plt.ylabel(\"Factors\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out 5 each of true and false positives as well as true and false negatives\n",
    "y_test_pred = gb.predict(X_test)\n",
    "print(\"Some True Positives: \" + str(np.where((y_test_pred == 1) & (y_test == 1))[0][:5])) # true positive\n",
    "print(\"Some True Positives: \" + str(np.where((y_test_pred == 0) & (y_test == 0))[0][:5])) # true negative\n",
    "print(\"Some False Positives: \" + str(np.where((y_test_pred == 1) & (y_test == 0))[0][:5])) # false positive\n",
    "print(\"Some False Negatives: \" + str(np.where((y_test_pred == 0) & (y_test == 1))[0][:5])) # false negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of each factor and it's coefficient estimate\n",
    "my_dictionary = {}\n",
    "coefficients = pd.Series(log.coef_[0])\n",
    "for key, value in zip(X.columns, coefficients):\n",
    "    my_dictionary[key] = value\n",
    "\n",
    "# Print results\n",
    "my_dictionary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
